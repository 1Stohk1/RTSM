# -*- coding: utf-8 -*-
"""data_analysis_alberto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15XXx0jB5kt9-PbobKsjCZRxYLvP4PJWo

# Time cycle: real vs. calculated
"""

from google.colab import drive
drive.mount('/content/drive/')
import pandas as pd
import numpy as np
df = pd.read_csv('/content/drive/MyDrive/SMART_APP/ML/data/P.csv')
# reformat dates
df['new_formatted_date'] = pd.to_datetime(df['ts'],format='%d/%m/%Y %H:%M')
df.head(10)

df['deltas'] = df.working_time - df['items']*df.cycle_time

df[df.deltas.abs()>1]

df['time_alive'] = (df.working_time+df.idle_time)

# only 4% of the dataset has a sum of working+idle different from 60s
(df.time_alive==60).sum()/len(df)

# only .16% of the dataset has real time cycle != measured time cycle
(df.deltas>.1).sum()/len(df)

# measured working time cycle
df['mwtc'] = df.working_time/df['items']
# measured total time cycle (working)
df['mttc'] = df.time_alive/df['items']
df.replace([np.inf, -np.inf], np.nan, inplace=True)

df[['mwtc','mttc']].hist(bins=10, histtype='step', stacked=True, fill=False)

import matplotlib.pyplot as plt

plt.hist([df['mttc'],df['mwtc']],
          bins=20, range=(1,60), stacked=True, fill=True,label=["(Working+Idle)/items","Working/items"],color=["orange","blue"],alpha=0.5)
plt.legend()
plt.title("Distribution of calculated time cycles")
plt.plot()

"""looking at the time difference only in the records where time_alive is 60 (working time+time_idle): [excluding machine turn on/tunrn off outlier cycles]"""

df[['time_alive','deltas']][df.time_alive==60][df.deltas>0]

"""# Part Program Separation"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import scipy.signal as sig

from google.colab import drive
drive.mount('/content/drive/')
df = pd.read_csv('/content/drive/MyDrive/SMART_APP/ML/data/P.csv')
# reformat dates
df['new_formatted_date'] = pd.to_datetime(df['ts'],format='%d/%m/%Y %H:%M')
df.head(10)

# suppongo che il tempo del part program si comporti come una gaussian mixture
df.cycle_time.hist(bins=120)
plt.yscale("log")

# distribution
data_of_interest = df.cycle_time[df.cycle_time>0]
n = np.isfinite(data_of_interest).sum()
d, bins = np.histogram(data_of_interest,bins=60);
ddf = pd.DataFrame(data = {'time': bins[:-1], 'p': (d+.1)/n})

"""ci sono 4 strategie che si considerano per per distinguere i diversi part program:

 1. deconvoluzione in dominio di frequenza
 2. gaussian mixture model, Expectation Maximization
 3. extreme deconvolution
 4. variational method for kernels extimation

## 1) frequency domain deconvolution
"""

ddf.p.plot()
plt.yscale('log')

#smoothing of the curve with standard signal processing methods
plt.plot(sig.savgol_filter(np.log(d+0.01), window_length=7, polyorder=3) )

#we assume that our distribution is the result of multiple dirac deltas convolved with a gaussian

#supposed gaussian
window_size=10
gaussian = sig.windows.gaussian(window_size, std=1)

#test
def test():
  test_in = np.array([0,0,0,0,0,.5,0,0,.5,0,0,0,0,0])
  test_conv = sig.convolve(test_in, gaussian*0.2)
  recovered, remainder = sig.deconvolve(test_conv, gaussian)
  plt.plot(test_in,label="Original Distribution")
  plt.plot(test_conv,label="`Blurred` Distribution")
  plt.plot(recovered,label="Recovered distribution")
  plt.plot(remainder,label="Computed Remainder")
  plt.legend()

test()

recovered, remainder = sig.deconvolve(ddf.p, gaussian)

plt.plot(np.log(recovered+.1))

# this signal processing method is way too sensible to Noise!

"""## 2) Gaussian Mixture Model with Expectation Maximization"""

import numpy as np
from sklearn.mixture import GaussianMixture

def gaussian_mixture_model(datarray,pp_num=2):
  data = datarray.values.reshape(-1,1)
  gm = GaussianMixture(n_components=pp_num, random_state=0,).fit(data)
  score = gm.score(data)
  gm.means_
  print("Converged:", gm.converged_,"; Weights:",gm.weights_,"; Score:", score)

  colors = [(.6,i,1-i,1) for i in np.arange(0,1,1/(len(gm.means_)+1))]
  plt.hist(datarray, bins=120)
  plt.plot
  for i, (color,mean) in enumerate(zip(colors,gm.means_)):
    plt.axvline(x = mean, color = color, label = i+1)
  plt.yscale("log")
  #plt.figsize(12,3)
  plt.legend()
  plt.show()
  return gm, score

def clusters_search(data, max_n):
  print("Iterating over the clusters range ...")
  results = {}
  for i in range(1,max_n):
    results[i] = [gaussian_mixture_model(data, i)]
    print("")
    

  nr = np.vstack([(c[0],c[1][0][1]) for c in results.items()])
  best_n = 2+np.argmax(np.diff(nr[:,1]))

  print("Choosing the best model: best_n")

  plt.plot(*nr.T,"-o",label="score")
  plt.plot(nr[1:,0],np.diff(nr[:,1]),label="iteration improvement")
  plt.axvline(x = best_n, color = "black", label = "Best Model")  

  plt.legend()
  plt.show()
  
  
  return results[best_n][0], results

"""### 2.1) GMM"""

(gm, score) ,_ = clusters_search(data_of_interest, 10)

gm, score

X = np.array(data_of_interest).reshape(-1,1)
Y = gm.predict(X)
np.unique(Y,return_counts=True)
plt.hist(X.flatten()[Y==0],label="0",alpha=1)
plt.hist(X.flatten()[Y==1],label="1",alpha=0.3)
plt.hist(X.flatten()[Y==2],label="2",alpha=0.3)
plt.gca().set_xlim((0,30))
plt.legend()
plt.show()

# molto piccata, meglio ma non e' ancora esattamente "robusta al rumore", si puo' anticipare con uno smoothing

"""### 2.2) Custom Gaussian Filter + GMM"""

from numpy import random

def my_gauss_filter(data,alpha=.3):
  s = data_of_interest.std()
  f = lambda x : random.normal(x, s*alpha)
  fv = np.vectorize(f)
  return fv(data)

filtered_data = pd.Series(my_gauss_filter(data_of_interest), name="smoothed_time_cycle")

filtered_data.hist(bins=100)
plt.title("Custom Gaussian Filter")
plt.show()

(gm, score), _ = clusters_search(filtered_data, 10)

gm3=_[3][0][0]

X = np.array(filtered_data).reshape(-1,1)
Y = gm3.predict(X)
np.unique(Y,return_counts=True)
plt.hist(X.flatten()[Y==0],label="0",alpha=1)
plt.hist(X.flatten()[Y==1],label="1",alpha=0.3)
plt.hist(X.flatten()[Y==2],label="2",alpha=0.3)
plt.gca().set_xlim((0,30))
plt.legend()
plt.show()

"""Note: the GMM produce a probabilistic separation function, which introduces some "tails" of outliers, this might be beneficial, or NOT... so we also provide a HARD separation

### 2.3) Hard separation (manual)
"""

# A, B = sig
#alpha*(A+B) = delta
#c_0 if x < XD.mu[0]+alpha*A
mu = gm3.means_[:2].flatten()
sig = gm3.covariances_[:2]
delta = float(np.abs(np.diff(mu)))
alpha = delta/sig.sum()
separation_point = float(mu[0] + alpha*sig[0])

output={
    "domain":"time",
    "splits":[separation_point],
    "model":gm3,
}
output

import pickle
with open("/content/drive/MyDrive/SMART_APP/ML/data/trained_part_program.model", 'wb') as f:
    pickle.dump(output, f)

plt.figure(figsize=(10,4))
plt.hist(data_of_interest[data_of_interest<separation_point],bins = 100)
plt.hist(data_of_interest[data_of_interest>separation_point],bins = 100)
plt.yscale('log')
plt.show()

df['part_program'] = np.zeros(len(df))

df.part_program[(df.cycle_time>1) & (df.cycle_time<separation_point)] = 1

df.part_program[df.cycle_time>separation_point] = 2

df.part_program.hist()

df.to_csv("/content/drive/MyDrive/SMART_APP/ML/data/P_with_part_program.csv")

"""## 3) XD [Extreme Deconvolution]

 https://doi.org/10.1214/10-AOAS439

 https://github.com/jobovy/extreme-deconvolution

### 3.1) Lib
"""

!pip install astroML

import numpy as np
from matplotlib import pyplot as plt

from astroML.density_estimation import XDGMM
from astroML.plotting.tools import draw_ellipse

from astroML.crossmatch import crossmatch
from astroML.datasets import fetch_sdss_S82standards, fetch_imaging_sample
from astroML.stats import sigmaG

# Sample the dataset. 
# Here we use sample size = 400 in the example, 
# which converges in shorter time, and gives reasonable result.
N = 400
np.random.seed(0)

# generate the true data
x_true = (1.4 + 2 * np.random.random(N)) ** 2
y_true = 0.1 * x_true ** 2

# add scatter to "true" distribution
dx = 0.1 + 4. / x_true ** 2
dy = 0.1 + 10. / x_true ** 2

x_true += np.random.normal(0, dx, N)
y_true += np.random.normal(0, dy, N)

# define a function to plot all distributions in the same format
def plot_distribution(text, sample_x, sample_y):
    plt.figure(figsize=(5, 3.75))
    plt.scatter(sample_x, sample_y, s=4,lw=0,c='k')
    plt.xlim(-1, 13)
    plt.ylim(-6, 16)
    plt.xlabel('$x$')
    plt.ylabel('$y$')
    plt.title(text,fontsize=10)

# plot true distribution
plot_distribution('True Distribution', x_true, y_true)

plt.show()

# add noise to get the "observed" distribution
dx = 0.2 + 0.5 * np.random.random(N)
dy = 0.2 + 0.5 * np.random.random(N)

x = x_true + np.random.normal(0, dx)
y = y_true + np.random.normal(0, dy)

# plot noisy distribution
plot_distribution('Noisy Distribution', x, y)

# stack the results for computation
X = np.vstack([x, y]).T
Xerr = np.zeros(X.shape + X.shape[-1:])
diag = np.arange(X.shape[-1])
Xerr[:, diag, diag] = np.vstack([dx ** 2, dy ** 2]).T

clf = XDGMM(n_components=10, max_iter=200)

clf.fit(X, Xerr)
sample = clf.sample(N)

X.shape

Xerr.shape

# plot noisy distribution
plot_distribution('Extreme Deconvolution Resampling', sample[:, 0], sample[:, 1])

"""### 3.2) Clustering"""

XD = XDGMM(n_components=2, max_iter=200)

X = data_of_interest.values.reshape(-1,1)

Xerr = np.zeros(X.shape + X.shape[-1:])

XD.fit(X, Xerr)

result = XD.sample(400)

XD.mu, XD.V[:,0]

"""### 3.3) 2 classes manual hard separation"""

# A, B = XD.V
#alpha*(A+B) = delta
#c_0 if x < XD.mu[0]+alpha*A
delta = XD.mu[1]-XD.mu[0]
alpha = delta/(XD.V.sum())
separation_point = float(XD.mu[0]+alpha*XD.V.flatten()[0])

plt.figure(figsize=(10,4))
plt.hist(result,bins = 100)
plt.axvline(x = separation_point, color = "red", label = "separation")

plt.figure(figsize=(10,4))
plt.hist(data_of_interest[data_of_interest<separation_point],bins = 100)
plt.hist(data_of_interest[data_of_interest>separation_point],bins = 100)
plt.yscale('log')
plt.show()

